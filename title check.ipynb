{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(types[i] + \"\\t:\\t\" + titles[i] + \"...\")\n",
    "\n",
    "g = open('titles.txt', 'r', encoding=\"utf8\") # read the titles\n",
    "titles = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('types.txt', 'r', encoding=\"utf8\") # What we WANT to know!\n",
    "types = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counts = Counter()\n",
    "#bs_counts = Counter()\n",
    "#bias_counts = Counter()\n",
    "#cap_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(types)):\n",
    "    #if(types[i] == 'bias'):\n",
    "        #for word in titles[i].split(\" \"):\n",
    "            #words_counts[word] += 1\n",
    "            #bs_counts[word] += 1\n",
    "   # else:\n",
    "        for word in titles[i].split(\" \"):\n",
    "            words_counts[word] += 1\n",
    "            #bias_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = set(words_counts.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_0 = np.zeros((1,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_target_for_label(label):\n",
    "    #\"\"\"Convert a label to `0` or `1`.\n",
    "    #Args:\n",
    "        #label(string) - Either \"POSITIVE\" or \"NEGATIVE\".\n",
    "    #Returns:\n",
    "        #`0` or `1`.\n",
    "    #\"\"\"\n",
    "    #if(label in ['bs', 'bias', 'satire', 'conspiracy', 'fake']):\n",
    "        #return 1\n",
    "    #else:\n",
    "        #return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Encapsulate our neural network in a class\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Assign a seed to our random number generator to ensure we get\n",
    "        # reproducable results during development \n",
    "        np.random.seed(1)\n",
    "\n",
    "        # process the reviews and their associated labels so that everything\n",
    "        # is ready for training\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        # Build the network to have the number of hidden nodes and the learning rate that\n",
    "        # were passed into this initializer. Make the same number of input nodes as\n",
    "        # there are vocabulary words and create a single output node.\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        # populate review_vocab with all of the words in the given reviews\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "\n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # populate label_vocab with all of the words in the given labels.\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "\n",
    "        # These are the weights between the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        # These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        # The input layer, a two-dimensional matrix with shape 1 x input_nodes\n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(\" \"):\n",
    "            # NOTE: This if-check was not in the version of this method created in Project 2,\n",
    "            #       and it appears in Andrew's Project 3 solution without explanation. \n",
    "            #       It simply ensures the word is actually a key in word2index before\n",
    "            #       accessing it, which is important because accessing an invalid key\n",
    "            #       with raise an exception in Python. This allows us to ignore unknown\n",
    "            #       words encountered in new reviews.\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "                \n",
    "    def get_target_for_label(label):\n",
    "        if(label in ['BS', 'BIAS', 'SATIRE', 'CONSPIRACY', 'FAKE']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        # Keep track of correct predictions to display accuracy during training \n",
    "        correct_so_far = 0\n",
    "\n",
    "        # Remember when we started for printing time statistics\n",
    "        start = time.time()\n",
    "        \n",
    "        # loop through all the given reviews and run a forward and backward pass,\n",
    "        # updating weights for every item\n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            # Get the next review and its correct label\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            # Keep track of correct predictions.\n",
    "            if(layer_2 >= 0.5 and (label in ['BS', 'BIAS', 'SATIRE', 'CONSPIRACY', 'FAKE'])):\n",
    "                correct_so_far += 1\n",
    "            #elif(layer_2 < 0.5 and label):\n",
    "                #correct_so_far += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the training process. \n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict\n",
    "        # its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the prediction process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "        # Run a forward pass through the network, like in the \"train\" function.\n",
    "        \n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"BS\"\n",
    "        else:\n",
    "            return \"real\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(titles[:-1000], types[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1601. #Correct:777 #Tested:1000 Testing Accuracy:77.7%"
     ]
    }
   ],
   "source": [
    "mlp.test(titles[-1000:],types[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'min_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-e7c01d3df13d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolarity_cutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_count'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
